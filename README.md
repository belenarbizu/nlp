# NLP Projects (Natural Language Processing)

This repository contains three projects focused on **text representation techniques** and their application in Natural Language Processing (NLP).  
Each notebook explores a different approach, from classical statistical methods to pre-trained deep learning models.

---

## Contents

1. **TF-IDF (Term Frequency â€“ Inverse Document Frequency)**  
   - File: `nlp_tfidf.ipynb`  
   - Description: Implementation of a classical text representation model.  
   - Goal: Transform documents into numerical vectors based on the relative importance of words within a corpus.  
   - Use cases: text classification, similarity analysis, information retrieval.  

2. **Word2Vec**  
   - File: `nlp_word2vec.ipynb`  
   - Description: Training of a neural network-based embedding model that captures semantic relationships between words.  
   - Goal: Represent words in a vector space where semantically similar words are located close to each other.  
   - Use cases: semantic analysis, word clustering, NLP task enhancement.  

3. **BERT (Bidirectional Encoder Representations from Transformers)**  
   - File: `nlp_bert.ipynb`  
   - Description: Usage of a deep language model based on **Transformers**, pre-trained on large amounts of text.  
   - Goal: Apply contextual embeddings that capture the meaning of words depending on their context.  
   - Use cases: text classification, sentiment analysis, question answering.  

---

## Technologies used

- Python  
- Scikit-learn  
- Gensim  
- Transformers (Hugging Face)  
- Jupyter Notebook  

---

## Repository goal

Compare different approaches to text representation in NLP, from traditional statistical methods to deep learning models, highlighting their strengths and limitations.
